# -*- coding: utf-8 -*-
"""DM Project Final.pynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ui-IKvZ092OXL7KtX0JGVcGjoz9_ZrDC
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
data = pd.read_csv('/content/twitter_sentiment_data.csv')
data.head()

# Assigning class labels - Pro, Anti, Neutral or News based on sentiment values
sentiment = data['sentiment']
sentiment_class = []
print(sentiment)
for ele in sentiment:
  if ele ==-1:
    sentiment_class.append('Anti')
  elif ele ==0:
    sentiment_class.append('Neutral')
  elif ele ==1:
    sentiment_class.append('Pro')
  elif ele ==2:
    sentiment_class.append('News')
data['sentiment'] = sentiment_class
data

#Extracting top 10 hashtags with highest frequency for every class 
import nltk
import re
def extract_hashtag(message):
   
    hashtags = []
    
    for i in message:
        hash = re.findall(r"#(\w+)", i)
        hashtags.append(hash)
        
    hashtags = sum(hashtags, [])
    frequency = nltk.FreqDist(hashtags)
    
    hashtagdf = pd.DataFrame({'hashtag': list(frequency.keys()),
                       'Frequency': list(frequency.values())})
    hashtagdf = hashtagdf.nlargest(10, columns="Frequency")

    return hashtagdf


Anti = extract_hashtag(data['message'][data['sentiment']=='Anti'])
Pro = extract_hashtag(data['message'][data['sentiment']=='Pro'])
Neutral = extract_hashtag(data['message'][data['sentiment']=='Neutral'])
News = extract_hashtag(data['message'][data['sentiment']=='News'])
Pro

# extracting all unique hashtags
hashtags = data['message'].apply(lambda x: re.findall(r'[#]\\w+',x))

hashtags = list(set([item for sublist in hashtags for item in sublist]))

# Lower case all words to remove noise from Capital words. Capital words may be seen as different from lower case words
data['message'] = data['message'].str.lower()
data

# Removing urls
data['message'] = data['message'].apply(lambda x: re.sub(r'https\S+','',x))
data['message'] = data['message'].apply(lambda x: re.sub(r'www\S+', '',x))
data

# Remove twitter non news related handles and @ symbol
data['message'] = data['message'].apply(lambda x: re.sub(r'@', '', ' '.join([y for y in x.split() if y not in 
                                                                                     [z for z in re.findall(r'@[\w]*',x) 
                                                                                      if z not in News]])))
data

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
def sentimentAnalysis(message):
    Sia = SentimentIntensityAnalyzer()
    score= Sia.polarity_scores(message)['compound']
    if score<-0.05:
            sentiment='negative'
    elif score>0.05:
            sentiment='positive'
    else:
            sentiment='neutral'
    return sentiment
    
# Add sentiment
data['message'] = data['message'].apply(lambda x: x + ' ' + sentimentAnalysis(x))

# Data Cleaning
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
def clean(tweet):
    
    # Convert to lowercase
    #tweet = tweet.lower()
    
    # Removing urls
    tweet = re.sub(r'https\S+','url',tweet)
    tweet = re.sub(r'www\S+', 'url',tweet)
    
    # Adding sentiment
    tweet = tweet + ' ' + sentimentAnalysis(tweet)
    
    # Removing punctuation
    tweet = re.sub(r"[^A-Za-z ]*",'',tweet)
    
    # Removing repeated vowels 
    tweet = re.sub(r'([aeiou])\1+', r'\1\1', tweet)

    #Removing stopwords
    #stop_words = stopwords.words('english')
    #if tweet not in stop_words:
    #   ' '.join(tweet)
    #tweet = tweet.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
    
    return tweet

data['message'] = data['message'].apply(lambda x: clean(x))
data



#Removing null values
def inds_nans(df):
    inds = df.isna().any(axis=1)
    return inds

#Removing duplicates
def inds_dups(df):
    inds = df.duplicated()
    return inds

data = data.loc[~(inds_nans(data)|inds_dups(data))]
data.shape

data['tokenized'] = data['message'].apply(word_tokenize)
data

#Lemmetization
from nltk.stem import WordNetLemmatizer
from nltk.corpus import  wordnet 
from nltk import word_tokenize
from nltk.tag import pos_tag
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')
def posttag(df):
   
    df['length'] = df['message'].str.len()
    df['tokenized'] = df['message'].apply(word_tokenize)
    df['pos_tags'] = df['tokenized'].apply(pos_tag)

    def get_wordnet_pos(tag):

        if tag.startswith('J'):
            return wordnet.ADJ

        elif tag.startswith('V'):
            return wordnet.VERB

        elif tag.startswith('N'):
            return wordnet.NOUN

        elif tag.startswith('R'):
            return wordnet.ADV
    
        else:
            return wordnet.NOUN
        
    
    df['pos_tags'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])
    print('pos tags',df['pos_tags'] )
    
    return df

df = posttag(data)
df.head()

from collections import Counter
wnl = WordNetLemmatizer()
df['lemmatized'] = df['pos_tags'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])
print('lemma', df['lemmatized'])
df['lemmatized'] = [' '.join(map(str, l)) for l in df['lemmatized']] 
count = Counter(df['lemmatized'])
df

#Top 10 most frequently used words in each sentiment

from sklearn.feature_extraction.text import CountVectorizer

def frequency(tweet):
    # Count vectorizer excluding english stopwords
    cv = CountVectorizer(stop_words='english')
    words = cv.fit_transform(tweet)

    sum_words = words.sum(axis=0)
    word_freq=[]
    for word, i in cv.vocabulary_.items():
       word_freq.append((word, sum_words[0, i]))
       word_freq = sorted(word_freq, key = lambda x: x[1], reverse = True)

    frequency = pd.DataFrame(word_freq, columns=['word', 'frequency of Occurence'])
    frequency = frequency.head(30)
    
    return frequency

# Extract the top 20 words in each class
prowords_frequency = frequency(df['lemmatized'][df['sentiment']=='Pro'])
antiwords_frequency = frequency(df['lemmatized'][df['sentiment']=='Anti'])
newswords_frequency = frequency(df['lemmatized'][df['sentiment']=='News'])
neutralwords_frequency = frequency(df['lemmatized'][df['sentiment']=='Neutral'])
prowords_frequency

"""Exploratory Data Analysis"""

import matplotlib.pyplot as plt

anti_words = ' '.join([text for text in antiwords_frequency['word']])
pro_words = ' '.join([text for text in prowords_frequency['word']])
neutral_words = ' '.join([text for text in neutralwords_frequency['word']])
news_words = ' '.join([text for text in newswords_frequency['word']])

from wordcloud import WordCloud
fig, axes = plt.subplots(ncols=2, nrows= 2, figsize=(10,8))


# make object of wordcloud
wc = WordCloud(background_color='white',min_font_size=10,width=500,height=500)
anti_words_plot = wc.generate(anti_words)
axes[0,0].imshow(anti_words_plot)

# make object of wordcloud
pro_words_plot = wc.generate(pro_words)
axes[0,1].imshow(pro_words_plot)

# make object of wordcloud
neutral_words_plot = wc.generate(neutral_words)
axes[1,0].imshow(neutral_words_plot)

# make object of wordcloud
news_words_plot = wc.generate(news_words)
axes[1,1].imshow(news_words_plot)

#data['lemmatized'] = data['lemmatized'].apply(lambda x: re.sub(r"[^A-Za-z ]*",'',x))

#Most- Freqently occuring words in Pro tweets
import seaborn as sns
from collections import Counter

# create list of True News words
true_news_words_list = df[df['sentiment']=='Pro']['lemmatized'].str.cat(sep=" ").split()

# create DataFrame of that
true_news_words_df = pd.DataFrame(Counter(true_news_words_list).most_common(20))


sns.barplot(x=true_news_words_df[0],y=true_news_words_df[1])
plt.xticks(rotation='vertical')
plt.xlabel('Words')
plt.ylabel('Counts')
plt.title('Most frequently occuring words in Pro')
plt.show()

data['sentiment'].value_counts()
import matplotlib.pyplot as plt
plt.figure(figsize = (7, 7))
plt.pie(data.sentiment.value_counts().values, labels = data.sentiment.value_counts().index, autopct = '%2.1f%%', textprops={'fontsize': 15})
plt.title('Sentiment Distribution of the Tweet Dataset', fontsize=20)
plt.tight_layout()
plt.show()

from sklearn.utils import resample
df = resample(df[sentiment]=='Anti',
             replace=True,
             n_samples=len(df[df[sentiment]=='News']),
             random_state=42)
df

# Create resampling function
def resampling(df, class1, class2):

    df_class1= df[df.sentiment==class1]
    df_class2 = df[df.sentiment==class2]
    df_new= df[df.sentiment!=class1]
    resampled = resample(df_class1, replace=True, n_samples=len(df_class2.sentiment), random_state=50)
    df_resampled = pd.concat([resampled, df_new])    
    return df_resampled

df_resample = resampling(data, -1, 2)
df_resample

import matplotlib.pyplot as plt
plt.figure(figsize = (7, 7))
plt.pie(df_resample.sentiment.value_counts().values, labels = df_resample.sentiment.value_counts().index, autopct = '%2.1f%%', textprops={'fontsize': 15})
plt.title('Sentiment Distribution of the Tweet Dataset', fontsize=20)
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(df_resample['meassage'], df_resample['sentiment'], train_size =0.8, random_state =0)

# create the transform
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words = 'english')

# transforming using TF-IDF
tfidf_train = vectorizer.fit_transform(X_train)
tfidf_train
tfidf_test = vectorizer.transform(X_test)
tfidf_test
tfidf_df = pd.DataFrame(tfidf_train.A, columns=vectorizer.get_feature_names())
tfidf_df

#K Nearest Neignbors Classifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, f1_score, ConfusionMatrixDisplay, recall_score
knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(tfidf_train, Y_train)
y_pred_test = knn.predict(tfidf_test)
print('Accuracy: %.3f' %accuracy_score(Y_test, y_pred_test))
print('Precision: %.3f' % precision_score(Y_test, y_pred_test, pos_label='positive', average='micro'))
print('Recall: %.3f' % recall_score(Y_test, y_pred_test, average='micro'))
print('F1 Score: %.3f' % f1_score(Y_test, y_pred_test, pos_label='positive', average='micro'))
ConfusionMatrixDisplay.from_estimator(knn, tfidf_test, Y_test)

# SVM Classifier
from sklearn import svm
from sklearn.metrics import accuracy_score, precision_score, f1_score, ConfusionMatrixDisplay, recall_score
svmclf = svm.SVC()
svmclf.fit(tfidf_train, Y_train)
y_pred_test = svmclf.predict(tfidf_test)
print('Accuracy: %.3f' %accuracy_score(Y_test, y_pred_test))
print('Precision: %.3f' % precision_score(Y_test, y_pred_test, pos_label='positive', average='micro'))
print('Recall: %.3f' % recall_score(Y_test, y_pred_test, average='micro'))
print('F1 Score: %.3f' % f1_score(Y_test, y_pred_test, pos_label='positive', average='micro'))
ConfusionMatrixDisplay.from_estimator(svmclf, tfidf_test, Y_test)

from sklearn.naive_bayes import GaussianNB
clfNB = GaussianNB()
clfNB.fit(tfidf_train,Y_train)
y_pred_test = clfNB.predict(tfidf_test)
print('Accuracy: %.3f' %accuracy_score(Y_test, y_pred_test))
print('Precision: %.3f' % precision_score(Y_test, y_pred_test, pos_label='positive', average='micro'))
print('Recall: %.3f' % recall_score(Y_test, y_pred_test, average='micro'))
print('F1 Score: %.3f' % f1_score(Y_test, y_pred_test, pos_label='positive', average='micro'))
ConfusionMatrixDisplay.from_estimator(svmclf, tfidf_test, Y_test)